\chapter{State of Art} \label{chapter:state_of_art}

The increasing complexities in managing \ac{lf} items spotlight the limitations of traditional approaches and the potential for intelligent solutions. This chapter explores the evolution of \ac{lfms}, focusing on manual methods, inventory management principles and, ultimately, the most relevant \ac{lf} platforms and solutions. By examining existing solutions, challenges, and best practices, this chapter lays the groundwork for designing a sturdy and efficient intelligent management system tailored to the demands of modern users.

%% ____________________ Manually Managing LF Items ____________________ %%

\section{Traditionally Managing \acl{lf} Items} \label{sec:manually-managing-lf-items}

Traditional \acp{lfms} typically relied on manual processes to log and manage items, sometimes papers or books and, in a few cases, spreadsheets \cite{Anas2023}. The fundamental components included physical logs, where details of found items - such as descriptions, location, and date of discovery - are recorded by staff or custodians.

Often, the responsibility of maintaining these records fell to a designated individual or department. Items are categorised and stored in a secure area, with the hope that owners would reclaim them. Matching found items to reported losses was a manual process, requiring significant time and effort \cite{Anas2023}. Descriptions provided by claimants are cross-referenced with recorded details to determine ownership. In some cases, rudimentary tagging systems are used to label items, aiding the identification process.

In environments like universities or corporate campuses, basic digital tools such as spreadsheets are usually introduced to track items. However, the overall workflow remains heavily dependent on manual oversight. Community bulletin boards, notices, or word-of-mouth are also standard methods of informing individuals about found items.

Despite their simplicity, these systems played a critical role in facilitating the return of lost belongings in the pre-digital era \cite{Mayura2024}. They promoted a sense of trust and collaboration within communities, relying on the goodwill and honesty of both finders and administrators. The traditional systems established the groundwork for modern approaches, providing valuable insights into the challenges and requirements of effective \ac{lf} management.

Despite the previously detailed inefficiencies of such systems, there are still those who would find these manual workflows to be more trustworthy. For some, the human element brings a level of accountability and understanding that automated systems cannot replicate. Manual processes allow for subjective judgment, which can be beneficial in complex scenarios where nuance is required. The tactile nature of handling paper documents or physical records stimulates a sense of security and reliability. Moreover, people who have had negative experiences with technology might prefer traditional methods, viewing them as more stable and less prone to glitches or failures.

%% ____________________ Challenges of Traditional LFMS ____________________ %%

While traditional \acp{lfms} have historically served their purpose in smaller or less demanding contexts, they face significant challenges when scaled to handle larger volumes of items or more complex environments \cite{Mayura2024}. The scalability of such systems is inherently constrained by their reliance on manual processes and both limited technological integration and automation.

A key obstacle is the dependency on human effort. As the number of items increases, so does the burden on the responsible staff, leading to delays, errors, and inefficiencies. In high-traffic environments, the sheer volume of items can quickly overwhelm even the most organised traditional systems \cite{Yao2019}. Without automation, processing and resolving claims becomes a time-intensive task, reducing the workflow's overall effectiveness. 

Another barrier to scaling is the lack of centralised data management. In traditional systems, records are often siloed, with each location or department maintaining its logs \cite{Soumya2024}. Especially in larger organisations or distributed campuses, the absence of a unified database also impedes efficient reporting and analysis of trends, such as identifying frequent loss locations or categories of items \cites{Sinha2024, Soumya2024}.

Communication between stakeholders presents further challenges. Traditional systems often lack any mechanisms for notifying individuals about found items or updating claimants on the status of their reports, resulting in unclaimed items \cite{Zhou2023} and frustrations, particularly in large-scale operations where the number of inquiries can be substantial.

Finally, the security of manual systems poses significant concerns. As the volume of items increases, the risk of theft, loss, or unauthorised access also rises. Inadequate labelling and verification processes can lead to disputes or errors in returning items to their rightful owners, further eroding trust in the system \cite{Guinard2008}.

%% ____________________ Inventory Management Systems ____________________ %%

\section{\aclp{ims}} \label{sec:ims}

\acp{ims} are tools designed to manage and track inventory levels across various domains. These systems enable organisations to streamline their operations, reduce costs, and enhance customer satisfaction by securing the availability of necessary items at the right time and place \cite{Pauliina2024}. An \ac{ims} employs methodologies to identify and classify inventory items based on details like quantity, volume, value or variability, optimising inventory performance through targeted strategies \cite{Pauliina2024}.

Fundamentally, an \ac{ims} aims to address some challenges, such as maintaining inventory levels, reducing holding costs, mitigating risks and many others associated with collections and stocks. Techniques like safety stock determination and lot-sizing methods have been widely adopted \cite{Prabakaran2023}. Such systems have historically evolved to incorporate some technological advancements, obtaining the capability to predict, plan, and execute inventory operations automatically and effectively \cite{Chebet2019}.

While conventional \acp{ims} primarily focus on physical products, their underlying principles can be effectively applied to tracking and managing \ac{lf} items. This intersection becomes particularly relevant in environments where the inventory (in this case, lost items) exhibits variability in value, volume, and retrieval demand. Inventory management principles, such as classification and optimisation, can be applied to \acp{lfms} to streamline processes and enhance user experiences. For instance, using the classification method mentioned before, in a \ac{lfms}, items could be categorised using an adapted ABC-XYZ framework \cite{Suryaputri2020} that enables prioritisation of storage, retrieval, and notification efforts \cite{Khobragade2018}. The Table \ref{tab:abc_xyz} illustrates a potential result of that association:

\begin{table}[H]
    \centering
    \caption{Classification of \ac{lf} items using the ABC-XYZ framework}
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        \textbf{Class} & \textbf{Value} & \textbf{Volume} & \textbf{Retrieval Demand} & \textbf{Examples} \\ 
        \hline
        A-X & High & Low & Consistent & Electronic devices, jewellery \\ 
        \hline
        B-Y & Moderate & Moderate & Fluctuating & Wallets, bags, clothing \\ 
        \hline
        C-Z & Low & High & Irregular & Umbrellas, stationery \\ 
        \hline
    \end{tabular}
    \label{tab:abc_xyz}
\end{table}

The insights from \citeauthoryear{Plinere2016} further support the integration of these methodologies into \acp{lfms}. Their case study emphasises the significance of structured inventory management practices for improving operational efficiency. For instance, high-priority items, such as class A-X items, can benefit from focused attention and expedited claim processes, guaranteeing user satisfaction while reducing storage costs. Furthermore, their findings highlight the value of predictive analytics in addressing slow-moving or stagnant inventory, matching to unclaimed items \cite{Plinere2016}. Technological integration, as illustrated in the case study, offers another avenue for improvement. The use of Radio-Frequency Identification (RFID) and Quick Response (QR) codes in conventional inventory systems ensures accurate tracking and categorisation of items \cite{Plinere2016, Sohail2018}. Moreover, the adoption of resource planning principles, such as imposing a centralised warehouse, would improve transparency and decision-making, possibly enabling a seamless synchronisation of data across departments. Lastly, the \citeauthoryear{Plinere2016} study underlines the importance of standardisation in inventory management policies, e.g., uniform intake procedures, categorisation standards, and clear guidelines for item disposition.

Adapting \acp{ims} for \ac{lf} management would definitely address some unique challenges \cite{Pauliina2024}. Unlike traditional inventory, lost items often have sentimental value or urgent retrieval needs, requiring the system to incorporate real-time tracking and user-friendly interfaces. Moreover, integrating predictive analytics, commonly used in \acp{ims} for forecasting demand, would be leveraged to anticipate peak periods of item loss or retrieval, e.g., events or seasons that may influence the volume and types of items lost, allowing proactive resource allocation \cite{Prabakaran2023}.

Building on these foundational principles, open-source \acp{ims} have evolved to address specific needs, blending traditional methodologies with modern technologies to enhance their adaptability and utility across domains. Odoo Inventory\footnote{\url{https://www.odoo.com/app/inventory}}, for instance, exemplifies this progression by integrating inventory management with enterprise-level tools like sales and customer relationship management. Its multi-warehouse support and barcode scanning capabilities make it a rich solution for medium to large enterprises. In contrast, Snipe-IT\footnote{\url{https://snipeitapp.com/demo}} focuses on a narrower domain (asset tracking), offering a well-designed interface for managing fixed assets, albeit without features like demand forecasting or multi-warehouse management. Moving further into specialised territories, InvenTree\footnote{\url{https://inventree.org/}} demonstrates how inventory systems can cater to engineering and manufacturing by providing tools for managing parts and components through hierarchical structures and batch tracking. However, it lacks the scalability offered by integration with other systems. Meanwhile, SkuNexus\footnote{\url{https://skunexus.com}} exemplifies the pivot toward e-commerce needs, combining advanced reporting, omnichannel order fulfilment, and process automation for complex operations, though its configurability demands significant effort during implementation. For small-scale, niche applications, PartKeepr\footnote{\url{https://partkeepr.org/}} offers essential inventory management tools, such as batch tracking and stock alerts, tailored to managing electronic components but without the scalability or advanced features needed for more extensive operations.

The following Table \ref{tab:ims_comparison} summarises the key features of these systems, emphasising their areas of strength and limitations:

\begin{table}[!htb]
    \centering
    \caption{Summarised comparison of open source \acl{ims} by features}
    \begin{tabular}{llllllll}
    \toprule
    \textbf{\ac{ims}} & \textbf{MWS} & \textbf{BI} & \textbf{DF} & \textbf{BT} & \textbf{ERPI} & \textbf{UFI} & \textbf{Total} \\
    \midrule
    Odoo & Yes & Yes & Yes & Yes & Yes & Yes & 6/6 \\
    Snipe-IT & No & No & No & No & No & Yes & 1/6 \\
    InvenTree & No & Yes & No & Yes & No & Yes & 3/6 \\
    SkuNexus & Yes & Yes & Yes & Yes & Yes & No & 5/6 \\
    PartKeepr & No & Yes & No & Yes & No & No & 2/6 \\
    \bottomrule
    \end{tabular}
    \caption*{\\MWS - Multi-Warehouse Support, BI - Barcode Integration, DF - Demand Forecasting, BT - Batch Tracking, ERPI - Enterprise Resource Planning Integration, UFI - User-Friendly Interface.}
    \label{tab:ims_comparison}
\end{table}

%% ____________________ Lost-And-Found Management Systems ____________________ %%

\section{In-Production \acl{lfms}} \label{sec:in-production-solutions}

The management of \ac{lf} items has evolved over the years, resulting in modern \acp{lfms} that leverage innovative features. This section delves into some of the most prominent \acp{lfms}, grouped by their functionality and strengths. All the systems that are going to be explored combine with integrated shipping and return subsystems, which expresses the importance of this feature in the context of \ac{lf} management.

\subsubsection{Comprehensive and Feature-Rich Solutions} \label{subsubsec:comprehensive-solutions}

NotLost\footnote{\url{https://notlost.com}} stands out as a highly versatile \ac{lfms}, offering a broad array of features that make it suitable for organisations of all sizes. Its strong automated matching and search capabilities, powered by some rudimentary artificial intelligence, simplify item identification and matching processes. The platform also emphasises data security and compliance, but despite its many strengths, NotLost lacks features for disposal and recycling, leaving room for improvement in managing unclaimed items.

Similarly, Chargerback\footnote{\url{https://www.chargerback.com}} offers an extensive feature set comparable to NotLost, with added emphasis on reporting and analytics and disposal and recycling management, making it an ideal choice for organisations that require comprehensive reporting tools to analyse \ac{lf} trends. Chargerback also has the best-found training and support, featuring online training and support, tech support, innovation support and a 24-hour quick response system for partners, which helps organisations onboard their staff effectively.

\subsubsection{Specialised Solutions for Targeted Needs} \label{subsubsec:specialised-solutions}

For organisations seeking user-centric platforms, iLost\footnote{\url{https://ilost.co}} and FoundHero\footnote{\url{https://foundhero.com}} offer intuitive, user-friendly interfaces that facilitate easy reporting and claiming of \ac{lf} items. While iLost shines in its focus on simplicity and efficiency for smooth item recovery, it lacks advanced features like automated matching and analytics. On the other hand, FoundHero emphasises customer feedback collection, which enables organisations to gather valuable insights from users on their \ac{lf} experiences.

Crowdfind\footnote{\url{https://www.crowdfind.com}} also prioritises usability, with a strong emphasis on visual tools such as photo-driven item searches. Its scalability and adaptability across multiple sectors make it a preferred choice for organisations managing large-scale \ac{lf} operations.

\subsubsection{Industry-Specific Solutions} \label{subsubsec:industry-specific-solutions}

ILeftMyStuff\footnote{\url{https://www.ileftmystuff.com}} caters specifically to the hospitality industry, offering specialised tools like automated guest communication via its communication tools. Its focus on these features, along with training and support, secures that hotels and similar establishments can manage \ac{lf} items without massive complaints and the need for intensive learning. However, the platform lacks support for advanced analytics and customer engagement features, limiting its broader applicability.

\subsubsection{Community-Driven and Volunteer-Based Solutions} \label{subsubsec:community-driven-solutions}

In contrast to enterprise-focused solutions, LostMyStuff\footnote{\url{http://www.lostmystuff.net/}} takes a community-driven approach. The platform connects individuals with volunteers to aid in recovering \ac{lf} items. While it lacks advanced technological features, its focus on volunteer and community engagement makes it unique in fostering a sense of shared responsibility and collaboration among users.

\subsubsection{Summary} \label{subsubsec:lfms_summary}

The Table \ref{tab:lfms_features} summarising the features of these \ac{lfms} platforms provides a detailed comparison of their features and capabilities, highlighting the strengths and limitations of each solution.

\begin{table}[!htb]
    \centering
    \caption{Feature Availability in \aclp{lfms}}
    \begin{tabular}{lllllllllllll}
        \toprule
        \textbf{System} & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
        \midrule
        NotLost & Yes & Yes & Yes & Yes & Yes & Yes & Yes & No & Yes & ? & ? & ? \\
        iLost & Yes & No & Yes & Yes & Yes & ? & ? & No & ? & ? & No & ? \\
        ILeftMyStuff & Yes & ? & Yes & Yes & Yes & ? & ? & No & ? & No & ? & Yes \\
        ReclaimHub & Yes & No & Yes & Yes & Yes & ? & No & Yes & Yes & No & ? & No \\
        Crowdfind & Yes & Yes & Yes & Yes & Yes & ? & Yes & ? & No & No & ? & ? \\
        Chargerback & Yes & Yes & Yes & Yes & Yes & ? & Yes & Yes & Yes & No & No & Yes \\
        MissingX & Yes & No & Yes & Yes & Yes & ? & Yes & Yes & Yes & ? & ? & ? \\
        FoundHero & Yes & No & Yes & Yes & Yes & Yes & ? & ? & Yes & ? & Yes & ? \\
        LostMyStuff & Yes & ? & Yes & ? & Yes & ? & ? & No & ? & Yes & ? & No \\
        \bottomrule
    \end{tabular}
    \caption*{\\\textbf{Yes} - Feature is included, \textbf{No} - Feature is not included, \textbf{?} - No information\\\textbf{1} - Item Logging and Inventory Management, \textbf{2} - Automated Matching and Search, \textbf{3} - User-Friendly Interfaces, \textbf{4} - Communication Tools, \textbf{5} - Integrated Shipping and Returns, \textbf{6} - Data Security and Compliance, \textbf{7} - Scalability and Multi-Sector Adaptability, \textbf{8} - Disposal and Recycling Management, \textbf{9} - Reporting and Analytics, \textbf{10} - Volunteer and Community Engagement, \textbf{11} - Customer Feedback Collection, \textbf{12} - Training and Support.}
    \label{tab:lfms_features}
\end{table}

%% ____________________ Systematic Literature Review ____________________ %%

\section{\acl{slr}} \label{chapter:literature-review}

This chapter summarises a \ac{slr} conducted using \ac{prisma} 2020 \cite{Page2021} guidelines\footnote{\url{https://static1.squarespace.com/static/65b880e13b6ca75573dfe217/t/676243b4c4a4752e13bbdfdf/1734493108989/PRISMA_2020_checklist.pdf}} to explore intelligent technologies for improving \ac{lfms}, emphasising advancements in \ac{ai} and \ac{ir}. The review addresses the research's objectives, including evaluating these technologies, identifying implementation challenges, and establishing best practices. This chapter also emphasises the progress in object recognition and multimodal matching and identifies scalability, computational, and ethical challenges, paving the way for future research to enhance \ac{lf} systems.

\subsection{Research Question and Methods} \label{subsec:slr}

\ac{lf} management has historically presented numerous challenges across both public and private sectors, as underlined in a wealth of academic articles and studies \cite{Prawira2024}. To address previously mentioned persistent issues and uncover effective solutions, a rigorous \ac{slr} was undertaken, focusing specifically on innovations, challenges, and best practices in the realm of intelligent lost property management. The review was designed with a methodological rigour that adheres to the \ac{prisma} 2020 checklist of items, guaranteeing that every aspect of the research was conducted with the highest standards of integrity and transparency. By synthesising the latest uncovered findings, the \ac{slr} offers not only a comprehensive understanding of the current landscape but also valuable insights that can inform the design of a \ac{lfms} system. The \ac{slr} later resulted in the production of a document directly aligned with this dissertation's scope, named \textit{"Designing an Intelligent Solution for Lost Property Management: A Systematic Review"}.

The review sought to answer the central research question \textit{"How can intelligent technologies enhance the efficiency and user experience of lost property management systems?"}, which refers to how \ac{ai}-based technologies such as \ac{cv} and \ac{nlp} and many others can provide solutions to the inefficiencies in \ac{lf} management. The \ac{slr} analysed numerous articles and integrated 18 high-quality studies that demonstrated the capacity to provide new insights into the investigation area. The mentioned research question was then separated into the following three major objectives:

\begin{itemize}
    \item Evaluating the applicability of the selected technologies;
    \item Examining existing challenges in implementing these technologies;
    \item Identifying best practices to inform the design and development of a proposed system.
\end{itemize}

The \ac{slr} employed a comprehensive four-phase approach associated with the selected \ac{prisma} framework, as illustrated in Figure \ref{fig:prisma_workflow}.

\begin{figure}[!htb]
    \includegraphics[width=\textwidth]{figs/chapter2/prisma.png}
    \centering
    \caption[\acs{prisma} 2020 flow diagram with a described quantity of research in each step.]{\acs{prisma} 2020 flow diagram with a described quantity of research in each step.}
    \label{fig:prisma_workflow}
\end{figure}

During the identification phase, academic databases such as Scopus and Web of Science were queried using targeted keywords like \ac{lf} Management, \ac{ims}, \ac{ai}, \ac{nlp}, and \ac{llm}, resulting in an initial yield of 476 studies. These were screened for relevance through the removal of duplicates and an abstract examination. In the eligibility phase, full-text articles were meticulously assessed against predefined inclusion criteria, which focused exclusively on studies published between 2020 and 2024 that addressed \ac{ai}-based solutions. Non-peer-reviewed works and studies lacking empirical validation were excluded. Additionally, to ensure the strength of the \ac{slr}, a quality assessment framework evaluated the methodological rigour and relevance of each study. Each study was rated on its technological contributions and practical applicability. Only those scoring consistently high across all criteria were included. Ultimately, in the inclusion phase, 18 studies were selected and categorised based on the technologies employed. This categorisation can be better analysed in Figure \ref{fig:prisma_results}, which includes the distribution of the technologies and frameworks explored and the most signigicant challenges pointed out. A thematic analysis was conducted, extracting valuable results into the contributions of each technological domain.

\begin{figure}[!htb]
    \begin{subfigure}{1\linewidth}
        \begin{tikzpicture}
            \pie[sum=auto,rotate=180,text=legend,radius=1.5]{13/\acs{yolo}, 4/\acs{resnet}-50, 1/Traditional Feature-Based Methods, 1/Cosine Similarity, 1/Euclidean Distance}
        \end{tikzpicture}
        \centering
        \caption{Technologies and Frameworks}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}{1\linewidth}
        \begin{tikzpicture}
            \pie[sum=auto,text=legend,radius=1.5]{5/Computational Intensity, 5/Dataset Quality Bias, 4/Scalability, 3/Multimodal Integration Complexity, 1/Ethical and Privacy Concerns, 1/Cost of Implementation and Integration}
        \end{tikzpicture}
        \centering
        \caption{Challenges}
    \end{subfigure}
    \centering
    \caption{Distribution of technologies, frameworks and challenges studied}
    \caption*{The figure illustrates the various technologies and frameworks utilized in the studies, alongside the primary challenges identified.}
    \label{fig:prisma_results}
\end{figure}

\subsection{Object Recognition and Categorisation} \label{subsec:object-recognition}

Object recognition and categorisation are processes in the domain of \ac{ai} and \ac{cv}. Object recognition involves identifying objects within an image or video and distinguishing them based on predefined features. Categorisation goes a step further by grouping identified objects into classes and/or groups of classes based on shared attributes or relationships \cite{Liu2021}. These tasks are foundational to numerous applications, including autonomous vehicles, facial recognition, surveillance systems, and many others.

The origins of object recognition can be traced back to the early experiments in pattern recognition during the 1950s and 1960s. Early approaches relied heavily on rule-based systems, where objects were identified using manually defined features such as edges, corners, or textures. \citeauthoryear{Marr1982}'s seminal work on computational vision in the 1980s introduced the concept of multi-level processing, exposing the importance of integrating both low-level (e.g., edge detection) and high-level (e.g., semantic) features. The field evolved significantly in the 1990s with the advent of statistical methods and \ac{ml}. Techniques like support vector machines and decision trees provided more robust frameworks for categorisation \cite{Bishop2006}. Concurrently, datasets such as MNIST\footnote{\url{https://yann.lecun.com/exdb/mnist/}} and ImageNet\footnote{\url{https://www.image-net.org/}} emerged, enabling standardised benchmarking and driving advancements in recognition accuracy.

The evolution of object recognition and categorisation has been marked by the rise of \ac{dl} in the 2010s. \acp{cnn}, such as AlexNet\footnote{\ac{cnn} architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton at the University of Toronto in 2012} and \ac{resnet}, models capable of leveraging feature extraction, revolutionised this field by achieving human-like performance on challenging tasks. These models finally enabled the recognition of complex patterns and relationships in data \cites{He2015, Krizhevsky2017}. More recently, transformer-based architectures, exemplified by vision transformers, have further enhanced the capabilities of recognition systems. These models utilise attention mechanisms to capture long-range dependencies and contextual information, surpassing the limitations of traditional \acp{cnn} \cite{Dosovitskiy2020}.

\acp{cnn}, particularly \ac{resnet}-50, are widely utilised due to their remarkable capability to extract visual features. Studies have shown that {resnet}-50 achieves high accuracy in identifying objects with varying attributes and appearances, a critical aspect for handling the heterogeneity of \ac{lf} items such as electronics, accessories, or apparel \cites{Prawira2024, Ghazal2016, Liu2022}. Similarly, \ac{yolo} models, including \ac{yolo}v7, are renowned for their real-time detection capabilities, ensuring low-latency processing and high precision even in challenging environments such as low light or cluttered backgrounds \cites{Sharma2024, Vedanth2024}.

\ac{resnet}-50, short for \acl{resnet} with 50 layers, has a \ac{dl} architecture designed to address the vanishing gradient problem in very deep neural networks. It introduces shortcut connections, or residual blocks, that allow gradients to flow more effectively during backpropagation, securing better convergence during training. \ac{resnet}-50 has become a standard in computer vision tasks due to its balance of depth and computational efficiency, making it suitable for extracting intricate visual features from diverse datasets \cite{He2015}. Its ability to generalise across object categories makes it a reliable choice for \ac{lf} management applications. \ac{yolo} is an object detection algorithm known for its speed and accuracy. Unlike traditional methods that scan an image region by region, \ac{yolo} processes the entire image in a single pass, predicting bounding boxes and class probabilities simultaneously, which significantly reduces computation time while maintaining high detection precision. \ac{yolo}v7, a more recent iteration, builds on these strengths by introducing architectural improvements for better performance in real-time scenarios, including high-density environments like traffic monitoring \cites{Redmon2015, Wang2022}.

Object recognition models can struggle with computational intensity, requiring significant resources for training and deployment \cites{Lubna2021, Mezhenin2021}. Lightweight versions of models are being developed to mitigate these issues, particularly for mobile applications, which are essential for systems designed to be universally accessible. Mobile-compatible frameworks, using optimised \ac{cnn}, provide the added advantage of enabling real-time item reporting and retrieval through user-friendly interfaces, expanding the reach of such systems \cites{Stout2024, Ghazal2016}.

Furthermore, these systems still need to contend with the diverse characteristics of objects. Items with subtle features or ambiguous shapes often pose difficulties for detection algorithms. Addressing this requires extensive and diverse datasets for training and validating in order to ensure that models can generalise effectively without overfitting to specific item categories \cites{Prawira2024, Liu2022, Sharma2024}. Hybrid approaches that integrate multiple algorithms or modalities are emerging as strategies to overcome these limitations. For instance, combining cloud-based data synchronisation with multimodal recognition has improved retrieval rates by facilitating large-scale processing and analysis \cite{Liu2024, Vedanth2024}.

\subsection{\acl{nlp}} \label{subsec:nlp}

\ac{nlp} is a multidisciplinary field at the intersection of linguistics, computer science, and artificial intelligence, enabling machines to process, understand, and generate human language. \ac{nlp} encompasses two core subfields: \ac{nlu} and \ac{nlg} \cite{Khurana2023}. \ac{nlu} focuses on interpreting and extracting meaning from textual or spoken language, including tasks such as sentiment analysis, intent recognition, and entity extraction, allowing systems to comprehend user input and respond accordingly \cite{Khurana2023}. In contrast, \ac{nlg} involves creating coherent and contextually appropriate textual or spoken output from structured data, such as generating summaries, reports, or conversational responses \cite{Dong2021}.

The origins of \ac{nlp} date back to the 1950s, when \citeauthoryear{Turing1950} proposed the concept of machine intelligence in his seminal work "Computing Machinery and Intelligence". One of the early milestones was the development of the Georgetown-IBM experiment in 1954, which demonstrated automatic translation between Russian and English, albeit limited to a small vocabulary and specific grammatical constructs. This marked the beginning of using computers to process and understand human language \cite{Hutchins2004}. About 50 years later, in the 1990s and early 2000s, \ac{ml} algorithms, mainly supervised learning, began to dominate \ac{nlp}, enhancing part-of-speech tagging, named entity recognition, and sentiment analysis. This era also witnessed the rise of the first large-scale resources for \ac{nlp}, including the Penn Treebank\footnote{https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html} and WordNet\footnote{https://wordnet.princeton.edu/}, which provided valuable training data and lexical knowledge \cite{Marcus1993, Fellbaum1998}.

More recently, the new era has been characterised by a revolution in \ac{nlp} fueled by \ac{dl}. Neural network architectures, particularly recurrent neural networks and their derivatives, long short-term memory networks, demonstrated remarkable capabilities in sequence-to-sequence tasks such as translation and text summarisation \cite{Bahdanau2015}. Furthermore, the introduction of attention mechanisms and transformer-based models, such as Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformer (GPT), has drastically improved the state of the art, enabling unprecedented performance across a wide range of \ac{nlp} tasks \cite{Vaswani2017, Devlin2019}. Today, \ac{nlp} continues to evolve, integrating cutting-edge advancements in \ac{ai}, including transfer learning and pre-trained language models, to achieve higher accuracy and efficiency in a variety of complex tasks, namely sentiment analysis, machine translation, and conversational agents \cite{Howard2018}.

The systematic review demonstrates the expanding role of \ac{nlp} in intelligent systems, particularly for enhancing human-computer interaction and automating complex processes. \ac{nlu} capabilities have become essential for interpreting user input, extracting key entities, sentiments, and intents from natural language queries. Modern \ac{lfms} leverage these advances to streamline item reporting and retrieval processes. \citeauthoryear{Prawira2024} and \citeauthoryear{Ghazal2016} have integrated \ac{nlu} models into their systems to process user descriptions into structured data that can be efficiently indexed and later matched against found items. Recent developments in large language models have further enhanced the capability to understand context, handle ambiguous descriptions, and extract meaningful semantic information from informal user inputs.

Contemporary \ac{nlp} systems face several challenges that impact their deployment in real-world applications. Scalability and computational efficiency remain primary concerns, particularly when deploying large transformer-based models in real-time environments with limited computational resources. The inference latency of modern language models can impede user experience in interactive systems. Furthermore, ethical considerations have gained prominence, including bias mitigation in language models, data privacy protection, and ensuring fair representation across diverse user populations. The responsible deployment of \ac{nlp} technologies requires careful consideration of these factors, particularly in systems that process personal information and descriptions of valuable items \cite{Prawira2024}.


\subsection{Multimodal Matching} \label{subsec:multimodal-matching}

Multimodal matching integrates diverse data modalities such as images, text, and audio for modern artificial intelligence applications. Advances in embedding techniques transform high-dimensional data into lower-dimensional vector spaces while preserving semantic relationships. Traditional approaches rely on modality-specific encoders: \acp{cnn} for image embeddings and transformer-based models for text embeddings \cite{He2015, Devlin2019}. However, recent developments have focused on unified architectures that can process multiple modalities within a single framework.

The evolution of vision-language models has been particularly influential in advancing multimodal matching capabilities. \ac{clip} learns joint representations of images and text through contrastive learning on large-scale web data \cite{Radford2021}. This methodology enables zero-shot classification and cross-modal retrieval by mapping visual and textual concepts into a shared embedding space. Building upon \ac{clip}'s foundation, subsequent models have addressed various limitations and expanded capabilities. Bootstrapping Language-Image Pre-training (BLIP) presents a unified multimodal framework that enables both understanding and generation through encoder-decoder components, incorporating synthetic caption generation to improve data quality \cite{Li2022}. Align Before Fuse (ALBEF) employs momentum distillation for data cleaning as its primary innovation, using momentum-updated pseudo-targets to filter noisy web data before alignment \cite{Li2021}.

Alternative architectures have explored different approaches to multimodal alignment. Foundation Language And Vision Alignment (FLAVA) presents a unified transformer architecture capable of processing vision-only, language-only, and multimodal tasks within a single model \cite{Singh2022}. Large-scale ImaGe and
Noisy-text embedding (ALIGN) scales contrastive learning to noisy web data, demonstrating that large-scale training can compensate for data quality issues \cite{Jia2021}. These models have established new benchmarks for cross-modal retrieval tasks, with performance improvements consistently observed across standard evaluation datasets.

Contrastive learning forms the primary approach for multimodal representation learning. The fundamental principle involves maximizing agreement between positive pairs (e.g., image-caption pairs) while minimizing agreement between negative pairs. Modern approaches have refined this methodology through various innovations. InfoNCE loss, originally introduced in the Contrastive Predictive Coding framework \cite{VanDenOord2018} and later adopted in SimCLR for visual representation learning \cite{Chen2020}, has been adapted for multimodal scenarios to improve the quality of learned representations. Recent developments include multi-level contrastive learning that operates at different granularities, from global image-text alignment to fine-grained token-level correspondences. Cross-modal attention mechanisms have been integrated to enable more nuanced alignment, allowing models to focus on relevant regions and concepts across modalities.

In the domain of \ac{lf} management, multimodal matching facilitates the alignment of user-provided queries with database entries through multiple channels. Visual embeddings derived from uploaded images are compared against stored item representations, while textual embeddings generated from user descriptions are matched against metadata and textual records. Modern approaches leverage the semantic understanding capabilities of vision-language models to bridge the gap between visual appearance and textual description. For instance, a user's natural language description of a "red leather wallet with multiple card slots" can be effectively matched against visual representations of similar items through learned cross-modal correspondences.

Advanced similarity metrics have been developed to improve matching accuracy beyond traditional cosine similarity and Euclidean distance measures. Probabilistic approaches that incorporate uncertainty estimation represent an active area of research, with potential applications in providing confidence intervals for decision-making processes. Learnable similarity functions that adapt to domain-specific characteristics remain an emerging research direction with potential benefits for specialized applications like lost item retrieval.

The scalability and efficiency of multimodal matching systems remain active areas of research. Techniques such as approximate nearest neighbor search \cite{Johnson2019}, quantization methods \cite{Jegou2011}, and hierarchical clustering have been employed to handle large-scale databases efficiently. Besides that, knowledge distillation approaches \cite{Hinton2015} enable the deployment of lightweight models while maintaining competitive performance. Furthermore, federated learning paradigms \cite{Li2020Fed} represent a promising research direction for addressing privacy concerns while potentially enabling collaborative matching across distributed systems.

Empirical evaluations demonstrate the effectiveness of state-of-the-art multimodal matching approaches. \citeauthoryear{Prawira2024} achieved high matching accuracy by integrating \ac{resnet} embeddings with cosine similarity for lost item identification. \citeauthoryear{Ghazal2016} demonstrated strong retrieval performance using a multi-feature image matching approach incorporating texture, shape, and colour features. Recent vision-language models demonstrate strong zero-shot performance on cross-modal retrieval benchmarks, with \ac{clip} achieving competitive results without task-specific training.

Despite these advances, several challenges persist in multimodal matching systems. Scalability concerns arise when deploying large vision-language models in resource-constrained environments. Bias mitigation remains crucial, as pre-trained models may inherit biases from their training data, potentially affecting fairness in matching decisions. The computational demands of transformer-based architectures necessitate careful optimization for real-time applications. Additionally, the cross-modal semantic gap continues to pose challenges, particularly when dealing with abstract concepts or domain-specific terminology that may not be well-represented in general-purpose training datasets.

While these state-of-the-art approaches represent the cutting edge of multimodal matching research, practical implementations often employ simplified architectures that balance performance with computational efficiency and deployment constraints. The gap between research prototypes and production systems highlights the ongoing need to translate advanced techniques into practical solutions that can operate reliably in real-world environments with limited computational resources.