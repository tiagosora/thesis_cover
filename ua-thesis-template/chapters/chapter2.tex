\chapter{State of Art}
\label{chapter:state_of_art}

\section{Manually Managing Lost-And-Found Items} \label{sec:manually-managing-lf-items}

\subsection{Traditional Management Systems} \label{subsec:traditional-lf-systems}

Traditional \acp{lfms} have been integral to managing misplaced belongings across various settings. These systems typically relied on manual processes to log and manage items, sometimes papers or books and, in a few cases, spreadsheets \cite{Anas2023}. The fundamental components included physical logs, where details of found items - such as descriptions, location, and date of discovery - were recorded by staff or custodians.

Often, the responsibility of maintaining these records fell to a designated individual or department. Items were categorised and stored in a secure area, with the hope that owners would reclaim them. Matching found items to reported losses was a manual process, requiring significant time and effort \cite{Anas2023}. Descriptions provided by claimants were cross-referenced with recorded details to determine ownership. In some cases, rudimentary tagging systems were used to label items, aiding the identification process.

In environments like universities or corporate campuses, basic digital tools such as spreadsheets were usually introduced to track items. However, the overall workflow remained heavily dependent on manual oversight. Community bulletin boards, notices, or word-of-mouth were also standard methods to inform individuals about found items.

Despite their simplicity, these systems played a critical role in facilitating the return of lost belongings in the pre-digital era \cite{Mayura2024}. They fostered a sense of trust and collaboration within communities, relying on the goodwill and honesty of both finders and administrators. The traditional systems established the groundwork for modern approaches, providing valuable insights into the challenges and requirements of effective lost-and-found management.

Despite the previously detailed inefficiencies of such systems, there are still those who would find these manual workflows to be more trustworthy. For some, the human element brings a level of accountability and understanding that automated systems cannot replicate. Manual processes allow for subjective judgment, which can be beneficial in complex scenarios where nuance is required. The tactile nature of handling paper documents or physical records fosters a sense of security and reliability. Moreover, people who have had negative experiences with technology might prefer traditional methods, viewing them as more stable and less prone to glitches or failures.


\subsection{Obstacles in Traditional Systems} \label{subsec:obstacles-traditional-systems}

While traditional \acp{lfms} have historically served their purpose in smaller or less demanding contexts, they face significant challenges when scaled to handle larger volumes of items or more complex environments \cite{Mayura2024}. The scalability of such systems is inherently constrained by their reliance on manual processes and both limited technological integration and automation.

A key obstacle is the dependency on human effort for recording, categorising, and matching items. As the number of lost items increases, so does the burden on staff, leading to delays, errors, and inefficiencies. In high-traffic environments, the sheer volume of items can quickly overwhelm even the most organised traditional systems. Without automation, processing and resolving claims becomes a time-intensive task, reducing the workflow's overall effectiveness. 

Another barrier to scaling is the lack of centralised data management. In traditional systems, records are often siloed, with each location or department maintaining its logs. Especially in larger organisations or distributed campuses, the absence of a unified database also impedes efficient reporting and analysis of trends, such as identifying frequent loss locations or categories of items.

Communication between stakeholders presents further challenges. Traditional systems often lack robust mechanisms for notifying individuals about found items or updating claimants on the status of their reports, resulting in inefficiencies and frustrations, particularly in large-scale operations where the number of inquiries can be substantial.

Finally, the security of manual systems poses significant concerns. As the volume of items increases, the risk of theft, loss, or unauthorised access also rises. Inadequate labelling and verification processes can lead to disputes or errors in returning items to their rightful owners, further eroding trust in the system.

These obstacles highlight traditional systems' limitations in adapting to the demands of modern \ac{lf} management. While effective in smaller, community-focused settings, their scalability issues underscore the need for innovative solutions that leverage technology to enhance efficiency, security, and user satisfaction.


\section{Inventory Management Systems} \label{sec:ims}

\acp{ims} are crucial tools designed to efficiently manage, track, and control inventory levels across various domains. These systems enable organisations to streamline their operations, reduce costs, and enhance customer satisfaction by ensuring the availability of necessary items at the right time and place \cite{Pauliina2024}. An \ac{ims} employs methodologies such as the ABC-XYZ analysis to classify inventory items based on value and variability, optimising inventory performance through targeted strategies \cite{Pauliina2024}.

Fundamentally, an \ac{ims} aims to address key challenges, including maintaining optimal inventory levels, reducing holding costs, and mitigating risks associated with stockouts or overstocking. Techniques like safety stock determination and lot-sizing methods have been widely adopted \cite{Prabakaran2023}. Such systems have historically evolved to incorporate technological advancements, enhancing the capability to predict, plan, and execute inventory operations effectively \cite{Chebet2019}.

While conventional \acp{ims} focus on tangible products, their principles are highly adaptable to managing intangible processes, such as \ac{lf} item tracking and management. This intersection becomes particularly relevant in environments where the inventory (in this case, lost items) exhibits variability in value, volume, and retrieval demand. Inventory management principles, such as classification and optimisation, can be applied to \acp{lfms} to streamline processes and enhance user experiences. For instance, using the classification method mentioned before, in a \ac{lfms}, items could be categorised using an adapted ABC-XYZ framework that enables prioritisation of storage, retrieval, and notification efforts \cite{Khobragade2018}. The Table \ref{table:abc_xyz} illustrates a potential result of that association:

\begin{table}[H]
\centering
\caption{Classification of \ac{lf} items using the ABC-XYZ framework}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Value} & \textbf{Volume} & \textbf{Retrieval Demand} & \textbf{Examples} \\ 
\hline
A-X & High & Low & Consistent & Electronic devices, jewellery \\ 
\hline
B-Y & Moderate & Moderate & Fluctuating & Wallets, bags, clothing \\ 
\hline
C-Z & Low & High & Irregular & Umbrellas, stationery \\ 
\hline
\end{tabular}
\end{table}

The insights from \citeauthoryear{Plinere2016} further support the integration of these methodologies into \acp{lfms}. Their case study emphasises the significance of structured inventory management practices for improving operational efficiency. For instance, high-priority items, such as class A-X items, can benefit from focused attention and expedited claim processes, ensuring user satisfaction while reducing storage costs. Furthermore, their findings highlight the value of predictive analytics in addressing slow-moving or stagnant inventory, matching to unclaimed items in \acp{lfms} \cite{Plinere2016}. Technological integration, as illustrated in the case study, offers another avenue for improvement. The use of \ac{rfid} and \ac{qr} codes in conventional inventory systems ensures accurate tracking and categorisation of items \cite{Plinere2016, Sohail2018}. Applying these technologies in a \ac{lf} context would improve the accuracy and speed of item retrieval, reducing manual effort and human error. Moreover, the adoption of resource planning principles, such as imposing a centralised warehouse, would improve transparency and decision-making, possibly enabling a seamless synchronisation of data across departments. Lastly, the \citeauthoryear{Plinere2016} study underlines the importance of standardisation in inventory management policies, e.g., uniform intake procedures, categorisation standards, and clear guidelines for item disposition.

Adapting \acp{ims} for \ac{lf} management would definitely address some unique challenges, improving identification, categorisation, and retrieval processes \cite{Pauliina2024}. Unlike traditional inventory, lost items often have sentimental value or urgent retrieval needs, requiring the system to incorporate real-time tracking and user-friendly interfaces. Moreover, integrating predictive analytics, commonly used in \acp{ims} for forecasting demand, would be leveraged to anticipate peak periods of item loss or retrieval, e.g., events or seasons that may influence the volume and types of items lost, allowing proactive resource allocation \cite{Prabakaran2023}.

Building on these foundational principles, open-source \acp{ims} have evolved to address specific needs, blending traditional methodologies with modern technologies to enhance their adaptability and utility across domains. Odoo Inventory\footnote{\url{https://www.odoo.com/app/inventory}}, for instance, exemplifies this progression by integrating inventory management with enterprise-level tools like sales and customer relationship management. Its multi-warehouse support and barcode scanning capabilities make it a robust solution for medium to large enterprises. In contrast, Snipe-IT\footnote{\url{https://snipeitapp.com/demo}} focuses on a narrower domain (asset tracking) offering a streamlined and user-friendly interface for managing fixed assets like IT equipment, albeit without features like demand forecasting or multi-warehouse management. Moving further into specialized territories, InvenTree\footnote{\url{https://inventree.org/}} demonstrates how inventory systems can cater to engineering and manufacturing by providing tools for managing parts and components through hierarchical structures and batch tracking. However, it lacks the scalability offered by integration with resource planning systems. Meanwhile, SkuNexus\footnote{\url{https://skunexus.com}} exemplifies the pivot toward e-commerce needs, combining advanced reporting, omnichannel order fulfillment, and process automation for complex operations, though its configurability demands significant effort during implementation. For small-scale, niche applications, PartKeepr\footnote{\url{https://partkeepr.org/}} offers essential inventory management tools, such as batch tracking and stock alerts, tailored to managing electronic components but without the scalability or advanced features needed for larger operations.

The following Table \ref{table:ims_comparison} summarizes the key features of these systems, highlighting their areas of strength and limitations:

\begin{table}[!htb]
\centering
\caption{Summarised comparison of open source \acl{ims} by features}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{\ac{ims}} & \textbf{MWS} & \textbf{BI} & \textbf{DF} & \textbf{BT} & \textbf{ERPI} & \textbf{UFI} \\
\hline
Odoo & Yes & Yes & Yes & Yes & Yes & Yes \\
\hline
Snipe-IT & No  & No  & No  & No  & No  & Yes \\
\hline
InvenTree & No  & Yes & No  & Yes & No  & Yes \\
\hline
SkuNexus & Yes & Yes & Yes & Yes & Yes & No  \\
\hline
PartKeepr & No  & Yes & No  & Yes & No  & No  \\
\hline
\end{tabular}
\caption*{\\MWS - Multi-Warehouse Support, BI - Barcode Integration, DF - Demand Forecasting, BT - Batch Tracking, ERPI - ERP Integration, UFI - User-Friendly Interface.}
\label{table:ims_comparison}
\end{table}

\section{\acl{slr}} \label{sec:literature-review}

\subsection{Research Question and Methods} \label{subsec:slr}

% TODO: Inserir referencias PRISMA 2020

\ac{lf} management has historically presented numerous challenges across both public and private sectors, as highlighted in a wealth of scholarly articles and studies \cite{Prawira2024}. To address previously mentioned persistent issues and uncover effective solutions, a rigorous \ac{slr} was undertaken, focusing specifically on innovations, challenges, and best practices in the realm of intelligent lost property management. The review was designed with a methodological rigour that adheres to the PRISMA 2020 guidelines, ensuring that every aspect of the research was conducted with the highest standards of integrity and transparency. By synthesising the latest uncovered findings, the \ac{slr} offers not only a comprehensive understanding of the current landscape but also crucial insights that can inform the development of efficient, user-centric systems. The \ac{slr} later resulted in the production of a document directly aligned with this dissertation's scope, named \textit{"Designing an Intelligent Solution for Lost Property Management: A Systematic Review"}.

The review sought to answer the central research question \textit{"How can intelligent technologies enhance the efficiency and user experience of lost property management systems?"}, which refers to how technologies such as \ac{ai}, \ac{nlp}, \ac{llm}, \ac{cv} and many others can provide solutions to the inefficiencies in \ac{lf} management. The \ac{slr} analysed numerous articles and integrated 18 high-quality studies that demonstrated the capacity to provide new insights into the investigation area. The mentioned research question was then separated into the following three major objectives:

\begin{itemize}
    \item Evaluating the applicability of the selected technologies in automating the identification, categorisation, and retrieval processes;
    \item Examining existing challenges, such as scalability, privacy concerns, and user accessibility, in implementing these technologies;
    \item Identifying best practices from the literature to inform the development of intelligent systems.
\end{itemize}

The \ac{slr} employed a comprehensive four-phase PRISMA 2020 approach. During the identification phase, academic databases such as Scopus and Web of Science were queried using targeted keywords like \ac{lf} Management, ac{ims}, ac{ai}, ac{nlp}, and ac{llm}, resulting in an initial yield of 476 studies. These were screened for relevance through the removal of duplicates and an abstract examination. In the eligibility phase, full-text articles were meticulously assessed against predefined inclusion criteria, which focused exclusively on studies published between 2020 and 2024 that addressed ac{ai}-based solutions. Non-peer-reviewed works and studies lacking empirical validation were excluded. Additionally, to ensure the robustness of the \ac{slr}, a quality assessment framework evaluated the methodological rigour and relevance of each study. Each study was rated on its technological contributions, scalability, and practical applicability. Only those scoring consistently high across all criteria were included. Ultimately, in the inclusion phase, 18 studies were selected and categorised based on the technologies employed. A thematic analysis was conducted, extracting valuable results into the contributions of each technological domain.

\subsection{Object Recognition and Categorisation} \label{subsec:object-recognition}

Object recognition and categorisation are pivotal processes in the domain of \ac{ai} and \ac{cv}. Object recognition involves identifying objects within an image or video and distinguishing them based on pre-defined features. Categorisation goes a step further by grouping identified objects into classes and/or groups of classes based on shared attributes or relationships \cite{Liu2021}. These tasks are foundational to numerous applications, including autonomous vehicles, facial recognition, surveillance systems, and many others.

The origins of object recognition can be traced back to the early experiments in pattern recognition during the 1950s and 1960s. Early approaches relied heavily on rule-based systems, where objects were identified using manually defined features such as edges, corners, or textures. \citeauthoryear{Marr1982}'s seminal work on computational vision in the 1980s introduced the concept of multi-level processing, emphasizing the importance of integrating both low-level (e.g., edge detection) and high-level (e.g., semantic) features. The field evolved significantly in the 1990s with the advent of statistical methods and \ac{ml}. Techniques like support vector machines and decision trees provided more robust frameworks for categorisation \cite{Bishop2006}. Concurrently, datasets such as MNIST\footnote{\url{https://yann.lecun.com/exdb/mnist/}} and ImageNet\footnote{\url{https://www.image-net.org/}} emerged, enabling standardised benchmarking and driving advancements in recognition accuracy.

The evolution of object recognition and categorisation has been marked by the rise of \ac{dl} in the 2010s. \acp{cnn}, such as AlexNet\footnote{\ac{cnn} architecture, designed by Alex Krizhevsky in collaboration with Ilya Sutskever and Geoffrey Hinton at University of Toronto in 2012} and \ac{resnet}, models capable of leveraging feature extraction, revolutionized this field by achieving human-like performance on challenging tasks. These models finally enabled the recognition of complex patterns and relationships in data \cites{He2015, Krizhevsky2017}. More recently, transformer-based architectures, exemplified by vision transformers, have further enhanced the capabilities of recognition systems. These models utilize attention mechanisms to capture long-range dependencies and contextual information, surpassing the limitations of traditional \acp{cnn} \cite{Dosovitskiy2020}.

\acp{cnn}, particularly \ac{resnet}-50, are widely utilised due to their capability to extract robust visual features. Studies have shown that ResNet-50 achieves high accuracy in identifying objects with varying attributes and appearances, a critical aspect for handling the heterogeneity of lost items such as electronics, accessories, or apparel \cites{Prawira2024, Ghazal2016, Liu2022}. Similarly, \ac{yolo} models, including \ac{yolo}v7, are renowned for their real-time detection capabilities, ensuring low-latency processing and high precision even in challenging environments such as low light or cluttered backgrounds \cites{Sharma2024, Vedanth2024}.

\ac{resnet}-50, short for \acl{resnet} with 50 layers, has a \ac{dl} architecture designed to address the vanishing gradient problem in very deep neural networks. It introduces shortcut connections, or residual blocks, that allow gradients to flow more effectively during backpropagation, ensuring better convergence during training. \ac{resnet}-50 has become a standard in computer vision tasks due to its balance of depth and computational efficiency, making it suitable for extracting intricate visual features from diverse datasets \cite{He2015}. Its ability to generalize across object categories makes it a reliable choice for lost item management applications. \ac{yolo} is an object detection algorithm known for its speed and accuracy. Unlike traditional methods that scan an image region by region, \ac{yolo} processes the entire image in a single pass, predicting bounding boxes and class probabilities simultaneously, which significantly reduces computation time while maintaining high detection precision. \ac{yolo}v7, a more recent iteration, builds on these strengths by introducing architectural improvements for better performance in real-time scenarios, including high-density environments like traffic monitoring \cites{Redmon2015, Wang2022}.

Object recognition models can struggle with computational intensity, requiring significant resources for training and deployment \cites{Lubna2021, Mezhenin2021}. Lightweight versions of models are being developed to mitigate these issues, particularly for mobile applications, which are essential for systems designed to be universally accessible. Mobile-compatible frameworks, using optimised \ac{cnn}, provide the added advantage of enabling real-time item reporting and retrieval through user-friendly interfaces, expanding the reach of such systems \cites{Stout2024, Ghazal2016}.

Furthermore, these systems still need to contend with the diverse characteristics of objects. Items with subtle features or ambiguous shapes often pose difficulties for detection algorithms. Addressing this requires extensive and diverse datasets for training and validating in order to ensure that models can generalise effectively without overfitting to specific item categories \cites{Prawira2024, Liu2022, Sharma2024}. Hybrid approaches that integrate multiple algorithms or modalities are emerging as strategies to overcome these limitations. For instance, combining cloud-based data synchronisation with multimodal recognition has improved retrieval rates by facilitating large-scale processing and analysis \cite{Liu2024, Vedanth2024}.

\subsection{\acl{nlp}} \label{subsec:nlp}

\ac{nlp} is a multidisciplinary field at the intersection of linguistics, computer science, and artificial intelligence, enabling machines to process, understand, and generate human language. \ac{nlp} encompasses two core subfields: \ac{nlu} and \ac{nlg} \cite{Khurana2023}. \ac{nlu} focuses on interpreting and extracting meaning from textual or spoken language, including tasks such as sentiment analysis, intent recognition, and entity extraction, allowing systems to comprehend user input and respond accordingly \cite{Khurana2023}. In contrast, \ac{nlg} involves creating coherent and contextually appropriate textual or spoken output from structured data, such as generating summaries, reports, or conversational responses \cite{Dong2021}.

The origins of \ac{nlp} date back to the 1950s, when \citeauthoryear{Turing1950} proposed the concept of machine intelligence in his seminal work "Computing Machinery and Intelligence". One of the early milestones was the development of the Georgetown-IBM experiment in 1954, which demonstrated automatic translation between Russian and English, albeit limited to a small vocabulary and specific grammatical constructs. This marked the beginning of using computers to process and understand human language \cite{Hutchins2004}. About 50 years later, in the 1990s and early 2000s, \ac{ml} algorithms, particularly supervised learning, began to dominate \ac{nlp}, enhancing part-of-speech tagging, named entity recognition, and sentiment analysis. This era also witnessed the rise of the first large-scale resources for \ac{nlp}, including the Penn Treebank\footnote{https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html} and WordNet\footnote{https://wordnet.princeton.edu/}, which provided valuable training data and lexical knowledge \cite{Marcus1993, Fellbaum1998}.

More recently, the new era has been characterized by a revolution in \ac{nlp} fueled by \ac{dl}. Neural network architectures, particularly recurrent neural networks and their derivatives, long short-term memory networks, demonstrated remarkable capabilities in sequence-to-sequence tasks such as translation and text summarization \cite{Bahdanau2015}. Furthermore, the introduction of attention mechanisms and transformer-based models, such as \ac{bert} and \ac{gpt}, has drastically improved the state of the art, enabling unprecedented performance across a wide range of \ac{nlp} tasks \cite{Vaswani2017, Devlin2019}. Today, \ac{nlp} continues to evolve, integrating cutting-edge advancements in \ac{ai}, including transfer learning and pre-trained language models, to achieve higher accuracy and efficiency in a variety of complex tasks, namely sentiment analysis, machine translation, and conversational agents \cite{Howard2018}.

The systematic review highlights the growing role of \ac{nlp} in intelligent systems, particularly in enhancing human-computer interaction and automating complex processes. For instance, \ac{nlu} is central to enabling systems to interpret user input, extracting key entities, sentiments, and intents. \citeauthoryear{Prawira2024} and \citeauthoryear{Ghazal2016}, have already adopted these capabilities in their \ac{lfms} to streamline the reporting and retrieval of lost items. By leveraging \ac{nlu} models, their systems can process user descriptions into structured data, that, once properly stored and indexed, can later be matched against found items.

Despite these advancements, several challenges remain. Scalability and computational efficiency are ongoing concerns, particularly when deploying \ac{nlp} models in real-time applications. Additionally, ethical considerations, including bias in language models and data privacy, require attention to ensure the responsible use of \ac{nlp} technologies \cite{Prawira2024}.


\subsection{Embeddings and Similarity Search} \label{subsec:object-recommendation}

Todo: Write this section.

\subsection{Privacy and Security} \label{subsec:privacy-security}

Todo: Write this section.

\subsection{User Experience} \label{subsec:user-experience}

Todo: Write this section.

% \subsection{Gamification} \label{subsec:gamification}

% Todo: Write this section.



\section{Designing an Intelligent \acl{ims} for \acl{lf}} \label{sec:designing-intelligent-ims}

\subsection{Optimal Approaches} \label{subsec:optimal-approaches}

Todo: Write this section.

\subsection{Best Practices} \label{subsec:best-practices}

Todo: Write this section.


\section{In-Production \acl{lfms}} \label{sec:on-production-solutions}

Todo: Write this section.
%% Item caracterization
%% NLU
%% Object recognition
%% Privacy and security
%% Introduction into IMS
%% User experience and gamification in the context of lost items


% \section{Summary} \label{sec:chapter2-summary}

% Todo: Write this section.
