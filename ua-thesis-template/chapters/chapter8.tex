\chapter{Deployment and Operations}
\label{chapter:deployment_operations}

% ____________________ Containerized Deployment Architecture ____________________ %

\section{Containerized Deployment Architecture} \label{section:container_deployment}

The deployment architecture brings together nine containerized services split between backend and frontend Docker Compose configurations. On the backend, seven services work in concert—FastAPI serves as the application's core, PocketBase handles data storage (port 8090), and Nginx acts as the reverse proxy. For observability, we deployed Prometheus (port 9090) to gather metrics, paired with Grafana (port 3000) for visualization. Node Exporter (port 9100) tracks system health while Nginx Exporter captures web server statistics. The frontend runs separately, hosting a React application on port 5173 alongside a Flutter web interface on port 8080.

The services connect in a hierarchical dependency chain where FastAPI needs PocketBase to be running, the monitoring stack watches FastAPI, and Nginx waits for both application services before accepting traffic. To keep everything running smoothly, health checks ping each service's HTTP endpoint at regular intervals—every 15 seconds for PocketBase and FastAPI, every 30 seconds for Nginx. When a service fails these checks, Docker automatically restarts it according to configured policies: \texttt{on-failure} for application services and \texttt{unless-stopped} for the monitoring stack.

We isolated the backend and frontend into separate Docker bridge networks for security. The \texttt{backend\_network} links all infrastructure services, letting them communicate internally by container name rather than IP address. During development, we expose backend service ports for debugging—FastAPI on 8000, PocketBase on 8090, Prometheus on 9090, Grafana on 3000, and Nginx on ports 80 and 443. Frontend services get their own \texttt{uachado-network}, with React accessible on port 5173 and Flutter on 8080.

Docker volumes handle data persistence, keeping information safe even when containers restart. PocketBase writes to \texttt{pb\_data}, Prometheus saves metrics to its named volume, and Grafana stores dashboards and settings in another persistent volume. We chose Alpine-based images to keep container sizes small and pinned specific versions in our Dockerfiles to guarantee consistent deployments across environments.

% ____________________ Monitoring and Observability ____________________ %

\section{Monitoring and Observability} \label{section:monitoring_observability}

We built monitoring into three layers to capture the complete system picture. Prometheus scrapes FastAPI's metrics endpoint every 5 seconds, giving us application-level insights into request handling and processing times. Docker's built-in metrics track container resource usage and health status across the deployment. For deeper system visibility, Node Exporter reads directly from the host's \texttt{/proc} and \texttt{/sys} directories (mounted read-only for security), capturing per-core CPU usage, memory allocation, disk space, and network traffic.

Grafana transforms these raw metrics into four dashboard views. The system overview tracks CPU, memory, and disk I/O across all services. Application performance dashboards show response times, throughput, and error rates. Business metrics reveal item lifecycles, user activity patterns, and claim resolution statistics. Alert panels highlight when any metric crosses its configured threshold.

Alert rules live in \texttt{/etc/prometheus/rules}, mounted as a directory in the Prometheus container. We set a 30-day retention window for metrics—old data gets pruned automatically to prevent disk exhaustion.

% ____________________ Operational Procedures ____________________ %

\section{Operational Procedures} \label{section:operational_procedures}

\subsection{Deployment Workflow} \label{subsection:deployment_workflow}

Deploying the system requires coordinating both backend and frontend environments. The backend pulls its configuration from a \texttt{.env} file containing database credentials, API keys, and feature flags. Frontend services get their environment variables directly from the Docker Compose file, particularly for development mode settings.

A typical deployment runs through three steps. First, \texttt{docker-compose config} validates that all configuration files parse correctly. Then \texttt{docker-compose build} constructs the container images from our Dockerfiles. Finally, \texttt{docker-compose up -d} launches everything in detached mode. The system waits for health checks to pass before routing any traffic to the newly deployed services.

\subsection{Configuration Management} \label{subsection:configuration_management}

Different configuration changes need different update procedures. When changing environment variables, a simple \texttt{docker-compose restart [service]} reloads the affected container with new values. Code changes demand rebuilding—first \texttt{docker-compose build [service]}, then \texttt{docker-compose up -d [service]} to deploy the updated image. Database schema migrations run automatically through PocketBase hooks that we place in the \texttt{pb\_hooks} directory.

\subsection{Data Persistence and Maintenance} \label{subsection:data_persistence_maintenance}

Docker volumes keep our data safe between container restarts and redeployments. PocketBase writes everything to \texttt{pb\_data}, while Prometheus and Grafana each have their own named volumes for metrics and dashboards. We track all configuration files in Git to keep deployments consistent across team members and environments.

Keeping the system secure means regularly updating the base images specified in our Dockerfiles, then rebuilding and redeploying any affected services. The 30-day Prometheus retention policy helps here too—it prevents disk space issues by automatically cleaning up old metrics data.

\subsection{Recovery Procedures} \label{subsection:recovery_procedures}

When services crash, Docker's restart policies kick in automatically. Application services use \texttt{on-failure} to restart after crashes, while monitoring services run with \texttt{unless-stopped} to stay available unless explicitly shut down. If services lose network connectivity to each other, tearing down and recreating the Docker network often fixes the problem—just run \texttt{docker-compose down} then \texttt{docker-compose up}.

\section{Summary} \label{section:deployment_summary}

This containerized deployment architecture successfully coordinates nine services between backend and frontend environments. Docker Compose handles the orchestration complexity—managing dependencies, running health checks, and isolating network traffic. The Prometheus and Grafana stack gives us visibility into system behavior through comprehensive metrics and dashboards. With clear operational procedures for deployment, configuration updates, and failure recovery, the system stays maintainable and resilient in production.